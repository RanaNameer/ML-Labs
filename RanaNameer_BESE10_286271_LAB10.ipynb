{
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Lab_multilayer_perceptron_on_MINST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "_cell_guid": "1d11b71f-50b4-333e-a4f9-df2fb192a40a",
        "id": "Lt4tlBknA3i5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal in this competition is to take an image of a handwritten single digit, and determine what that digit is.  \n",
        "\n",
        "The data is taken from the MNIST dataset. The MNIST (\"Modified National Institute of Standards and Technology\") dataset is a classic within the Machine Learning community that has been extensively studied.  More detail about the dataset, including Machine Learning algorithms that have been tried on it and their levels of success, can be found [here][1].\n",
        "\n",
        "\n",
        "  [1]: http://yann.lecun.com/exdb/mnist/index.html"
      ],
      "metadata": {
        "_cell_guid": "1e0a81f5-8486-c1ca-f5c5-bc723111da99",
        "id": "DWlVOdxWA3i-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the data"
      ],
      "metadata": {
        "_cell_guid": "c7313b40-f504-5443-a468-008df68f59bb",
        "id": "EusAKEgvA3jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Array manipulation\n",
        "import pandas as pd # Dataframe manipulation\n",
        "import tensorflow as tf\n",
        "# Multilayer perceptron Neural Network\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import np_utils"
      ],
      "metadata": {
        "_cell_guid": "ca99747a-0397-57e6-7129-375b1a05bd37",
        "execution": {
          "iopub.status.busy": "2022-02-25T07:59:09.746507Z",
          "iopub.execute_input": "2022-02-25T07:59:09.746876Z",
          "iopub.status.idle": "2022-02-25T07:59:15.897247Z",
          "shell.execute_reply.started": "2022-02-25T07:59:09.746837Z",
          "shell.execute_reply": "2022-02-25T07:59:15.896099Z"
        },
        "trusted": true,
        "id": "ORPV640oA3jC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "XHkpwk1s3xYD",
        "outputId": "0ec17cbd-4018-47b8-ef79-82a30fd41fd2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0a3d9dc0-1bec-4cce-bf2f-479f418d7682\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0a3d9dc0-1bec-4cce-bf2f-479f418d7682\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.csv to train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train = pd.read_csv('train.csv') \n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "_cell_guid": "fa85554d-254d-fa59-4d9b-1499d77ef248",
        "execution": {
          "iopub.status.busy": "2022-02-25T07:59:15.899007Z",
          "iopub.execute_input": "2022-02-25T07:59:15.899284Z",
          "iopub.status.idle": "2022-02-25T07:59:21.844019Z",
          "shell.execute_reply.started": "2022-02-25T07:59:15.899252Z",
          "shell.execute_reply": "2022-02-25T07:59:21.842975Z"
        },
        "trusted": true,
        "id": "SjrCvIaqA3jF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the features matrix X and transform it to an array of float numbers. And also extract the labels."
      ],
      "metadata": {
        "_cell_guid": "7af5f2b3-0855-5978-7d04-4da5974cf9d2",
        "id": "bO3I3_tZA3jH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract images pixels\n",
        "images = train.iloc[:,1:].values\n",
        "images = images.astype(np.float)\n",
        "\n",
        "# Extract numbers Labels\n",
        "labels = train.iloc[:,0].values"
      ],
      "metadata": {
        "_cell_guid": "5f0ab393-676b-74e8-a898-c3b32954b5c8",
        "execution": {
          "iopub.status.busy": "2022-02-25T07:59:24.374843Z",
          "iopub.execute_input": "2022-02-25T07:59:24.375164Z",
          "iopub.status.idle": "2022-02-25T07:59:24.501773Z",
          "shell.execute_reply.started": "2022-02-25T07:59:24.375130Z",
          "shell.execute_reply": "2022-02-25T07:59:24.500487Z"
        },
        "trusted": true,
        "id": "VQGGe35WA3jI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d51142-0806-4050-e8dd-179f0cfad272"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multilayer Perceptron"
      ],
      "metadata": {
        "_cell_guid": "64ce8b1d-bc53-14ea-bd99-e9dc559c3283",
        "id": "DJVUJsiZA3jJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "_cell_guid": "888840e0-cddb-2173-13f4-5f2b07c4b296",
        "id": "uI9NEg2lA3jK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pixel values are gray scale between 0 and 255. It is almost always a good idea to perform some scaling of input values when using neural network models. Because the scale is well known and well behaved, we can very quickly **normalize** the pixel values to the range 0 and 1 by dividing each value by the maximum of 255.\n",
        "\n",
        "Also, the output variable is an integer from 0 to 9. This is a multi-class classification problem. As such, it is good practice to use a **one hot encoding** of the class values, transforming the vector of class integers into a binary matrix. We can easily do this using the built-in np_utils.to_categorical() helper function in Keras."
      ],
      "metadata": {
        "_cell_guid": "7f7709ce-9b5f-f45d-c583-1a49f7f733df",
        "id": "Abi8wVdrA3jL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize input from 0-255 to 0-1\n",
        "images = images / 255.0\n",
        "num_pixels =  images.shape[1]\n",
        "\n",
        "# one hot encode outputs\n",
        "labels = np_utils.to_categorical(labels)\n",
        "num_classes = labels.shape[1]"
      ],
      "metadata": {
        "_cell_guid": "1dcb5c59-bd89-9fbc-e3e4-275ba0030cf0",
        "execution": {
          "iopub.status.busy": "2022-02-25T07:59:46.467629Z",
          "iopub.execute_input": "2022-02-25T07:59:46.467909Z",
          "iopub.status.idle": "2022-02-25T07:59:46.584907Z",
          "shell.execute_reply.started": "2022-02-25T07:59:46.467876Z",
          "shell.execute_reply": "2022-02-25T07:59:46.583989Z"
        },
        "trusted": true,
        "id": "5CFVehsaA3jL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to create our simple neural network model. We will define our model in a function. This is handy if you want to extend the example later and try and get a better score.\n",
        "\n",
        "The model is a **simple neural network** with **one hidden layer** with the same **number of neurons as there are inputs (784)**. A **rectifier activation function** is used for the neurons in the hidden layer.\n",
        "\n",
        "A **softmax activation function** is used on the output layer to turn the outputs into probability-like values and allow one class of the 10 to be selected as the modelâ€™s output prediction. **Logarithmic loss** is used as the loss function (called **categorical_crossentropy** in Keras) and the efficient **ADAM gradient descent algorithm** is used to **learn the weights**."
      ],
      "metadata": {
        "_cell_guid": "2570ea91-a644-a6f0-d943-f9e6d6b66783",
        "id": "j5IF_A9UA3jM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "_cell_guid": "5e27495d-5a10-f00c-2e73-25507f928d6b",
        "id": "8Aub_WzJA3jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define baseline model\n",
        "def mlp_model():\n",
        "\n",
        "\t\t# create model\n",
        "\t\t#TODO\n",
        "\t\tmodel = tf.keras.models.Sequential()\n",
        "\t\tmodel.add(Dense(784,activation=tf.keras.activations.relu))\n",
        "\t\tmodel.add(tf.keras.layers.Dense(\n",
        "\t\tunits=10,\n",
        "\t\tactivation=tf.keras.activations.softmax))\n",
        "\n",
        "\t\t# Compile model\n",
        "\t\t#TODO\n",
        "\t\t#adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "\t\tmodel.compile(optimizer='Adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\t\treturn model"
      ],
      "metadata": {
        "_cell_guid": "31980f83-aaf8-a086-9494-de28ea2a9e54",
        "execution": {
          "iopub.status.busy": "2022-02-25T08:03:54.892807Z",
          "iopub.execute_input": "2022-02-25T08:03:54.893089Z",
          "iopub.status.idle": "2022-02-25T08:03:54.899843Z",
          "shell.execute_reply.started": "2022-02-25T08:03:54.893059Z",
          "shell.execute_reply": "2022-02-25T08:03:54.898780Z"
        },
        "trusted": true,
        "id": "r_U5V5A6A3jN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now fit and evaluate the model. The model is fit **over 10 epochs with updates every 200 images**. A verbose value of 2 is used to reduce the output to one line for each training epoch."
      ],
      "metadata": {
        "_cell_guid": "7c0ebc00-beb4-e159-3657-3b84579a8a1e",
        "id": "3cvritYhA3jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model\n",
        "model = mlp_model()\n",
        "# Fit the model\n",
        "model.fit(images,labels,\n",
        "          batch_size=200,\n",
        "          epochs=10,\n",
        "          verbose=2)"
      ],
      "metadata": {
        "_cell_guid": "6961e1b4-bbac-c951-d3d4-9a36684d6d6a",
        "execution": {
          "iopub.status.busy": "2022-02-25T08:03:57.953932Z",
          "iopub.execute_input": "2022-02-25T08:03:57.954522Z",
          "iopub.status.idle": "2022-02-25T08:03:58.013117Z",
          "shell.execute_reply.started": "2022-02-25T08:03:57.954483Z",
          "shell.execute_reply": "2022-02-25T08:03:58.011808Z"
        },
        "trusted": true,
        "id": "MhaaMiHkA3jO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5272d3-ccb4-43bd-8068-53afe3174acd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "210/210 - 3s - loss: 0.3179 - accuracy: 0.9113 - 3s/epoch - 14ms/step\n",
            "Epoch 2/10\n",
            "210/210 - 2s - loss: 0.1312 - accuracy: 0.9625 - 2s/epoch - 11ms/step\n",
            "Epoch 3/10\n",
            "210/210 - 2s - loss: 0.0878 - accuracy: 0.9754 - 2s/epoch - 10ms/step\n",
            "Epoch 4/10\n",
            "210/210 - 2s - loss: 0.0599 - accuracy: 0.9826 - 2s/epoch - 10ms/step\n",
            "Epoch 5/10\n",
            "210/210 - 2s - loss: 0.0441 - accuracy: 0.9878 - 2s/epoch - 10ms/step\n",
            "Epoch 6/10\n",
            "210/210 - 2s - loss: 0.0326 - accuracy: 0.9914 - 2s/epoch - 10ms/step\n",
            "Epoch 7/10\n",
            "210/210 - 2s - loss: 0.0245 - accuracy: 0.9940 - 2s/epoch - 11ms/step\n",
            "Epoch 8/10\n",
            "210/210 - 2s - loss: 0.0187 - accuracy: 0.9957 - 2s/epoch - 10ms/step\n",
            "Epoch 9/10\n",
            "210/210 - 2s - loss: 0.0134 - accuracy: 0.9975 - 2s/epoch - 10ms/step\n",
            "Epoch 10/10\n",
            "210/210 - 2s - loss: 0.0107 - accuracy: 0.9981 - 2s/epoch - 10ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6ec4fced0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we predict the model, we change our one hot encoded (binary matrix) results to a vector of labels from 0 to 9, and we save our results in a submission file"
      ],
      "metadata": {
        "_cell_guid": "7ea4fac4-5b4e-427c-cd7c-255c87f128f4",
        "id": "EDppEiTFA3jP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "_cell_guid": "1a9e8575-f459-a7e2-2a0d-0cfe7998ea48",
        "id": "sTVq5KJjA3jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import joblib\n",
        "\n",
        "saved_model = pickle.dumps(model)\n",
        "joblib.dump(model, 'model.pkl', compress=9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQCrGxgdvysr",
        "outputId": "a592e01c-e246-4216-beff-47f9b3111cf2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://d41d128e-59b5-44e4-9a84-5f0ac54cee24/assets\n",
            "INFO:tensorflow:Assets written to: ram://7a18c1e3-a132-4f37-ad84-8a7d15d22f18/assets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}